
The System Analysis phase is essential in defining the structure, features, and requirements of the Emotionally-Aware Presentation Analyzer, a component of the SpeaKraft platform. The primary aim of this module is to help Sri Lankan undergraduates improve their presentation skills by evaluating whether their facial emotions are visible, consistent, and aligned with the spoken content. By addressing the limitations of traditional training approaches that provide feedback only on speech clarity or filler words, this system seeks to bridge the gap in emotional awareness and delivery that is often overlooked in presentation skill development.

A central aspect of this system analysis is identifying the functional requirements that the module must incorporate in order to be effective. These requirements include a reliable mechanism for continuous face detection and visibility tracking, a robust CNN-based emotion recognition model capable of classifying facial expressions with high accuracy, and an NLP-driven content analysis module that interprets the tone and context of the presentation text. By combining these components, the system will be able to assess not only the emotions expressed by the presenter but also whether those emotions are congruent with the content being delivered. The system will also provide a post-presentation analytical report, summarizing visibility, emotional timelines, congruence scores, and targeted recommendations for improvement.
In terms of development methodology, the system will follow an iterative and incremental approach, ensuring that feedback from test users—primarily undergraduate students and academic supervisors—can be integrated throughout the design and implementation stages. This participatory process will help refine the system so that it remains culturally relevant, technically accurate, and pedagogically valuable. Stakeholder engagement will be particularly important, as feedback from students and educators will guide adjustments to ensure usability, clarity of reports, and alignment with real presentation scenarios.
The system will need to incorporate several core features to achieve its objectives. First, the face visibility tracker will ensure that the presenter remains within the camera frame, as partial or inconsistent visibility reduces audience engagement. Second, the emotion recognition engine will classify emotions such as happiness, sadness, anger, fear, surprise, and neutrality using CNN models adapted through transfer learning for Sri Lankan undergraduates. Third, the content-emotion alignment module will compare the expected tone of the presentation text with the detected emotional state to determine whether delivery was authentic and appropriate. Together, these features will provide a comprehensive assessment of emotional delivery.
Beyond analysis, the system will include real-time and post-presentation feedback mechanisms. During live presentations, optional alerts will notify presenters if their face is not visible or if emotional mismatches occur, allowing for immediate correction. At the end of the presentation, an analytics dashboard and exportable report will present detailed metrics, including emotion timelines, congruence heatmaps, visibility percentages, and personalized improvement tips. This dual feedback approach ensures that students can learn both in the moment and through reflective practice afterward.
The implementation strategy will unfold in several stages. Initially, a prototype of the Analyzer will be developed and tested in controlled environments using small groups of students. Feedback during this phase will be used to refine the accuracy of detection models and the clarity of reports. Once validated, the system will undergo larger-scale pilot testing in university classrooms and presentation workshops. This staged implementation will ensure that the system integrates smoothly into academic contexts and genuinely supports the improvement of undergraduate presentation skills.
Ultimately, the educational impact of this system lies at the heart of the analysis. By focusing on face visibility, emotional recognition, and emotion-content alignment, the Emotionally-Aware Presentation Analyzer aims to create a structured, culturally adapted, and technologically advanced environment for presentation training. Its feedback mechanisms will not only make students aware of their emotional delivery but will also provide clear pathways for improvement. In doing so, the system seeks to help Sri Lankan undergraduates build confidence, authenticity, and professionalism in their communication, bridging the gap between content knowledge and effective delivery.




The software solution approach defines the overall architecture, development methodology, and implementation strategy for the Emotionally-Aware Presentation Analyzer. This component of the SpeaKraft platform is designed to assess the visibility of a presenter’s face, classify emotions using convolutional neural networks, and align these emotions with the spoken content of a presentation. The approach emphasizes scalability, functionality, user experience, and maintainability to ensure that the system is both technically robust and pedagogically valuable for Sri Lankan undergraduates.

The first step involves systematic data collection to understand the unique challenges that students face in emotional delivery during presentations. Insights will be gathered through classroom observations, structured questionnaires, and focus group discussions with both students and academic supervisors. This process will provide an understanding of the recurring difficulties in maintaining visibility, expressing appropriate emotions, and aligning emotional delivery with the content being presented. The qualitative and quantitative data collected will guide the design of the system’s requirements and ensure that the final solution is aligned with the practical realities of presentation training in Sri Lankan universities.

The next stage focuses on facial detection and visibility monitoring. Instead of relying on general-purpose APIs, the system integrates a dedicated face recognition pipeline built with OpenCV and deep learning models to ensure consistent tracking of the presenter’s face throughout the presentation. By continuously monitoring framing, lighting, and visibility, the module ensures that the audience has uninterrupted access to facial cues, which are critical for interpreting emotional intent. This stage also supports the reliability of the emotion recognition process by guaranteeing that facial inputs are of sufficient quality.

The core of the solution lies in the emotion recognition model. A convolutional neural network will be trained to classify facial expressions such as happiness, sadness, anger, surprise, and neutrality. To overcome the challenges of limited local data, transfer learning will be applied, allowing the model to adapt pre-trained architectures such as VGG16 or ResNet to the Sri Lankan undergraduate context. Image preprocessing and augmentation techniques will be incorporated to improve accuracy under varying lighting and environmental conditions. The training process will focus on reducing cultural bias by including locally collected datasets that reflect the diversity of emotional expressions among Sri Lankan students.

To evaluate emotional authenticity, the Analyzer integrates natural language processing for content analysis. Speech-to-text conversion will generate transcripts of presentations, which will then be segmented and analyzed to determine the expected tone of each section. The recognized emotions from the CNN model will be compared with the semantic interpretation of the text to assess congruence. For example, a motivational section may be expected to carry enthusiasm, while a discussion of social problems may require a more serious tone. This cross-modal analysis will form the foundation of the system’s feedback mechanism.

The system will also include a reporting and analytics framework. After each presentation, students will receive a comprehensive analytical report that outlines face visibility percentages, an emotion timeline, congruence scores, and actionable recommendations for improvement. The user interface will be designed with React and Vite to ensure that the reports are clear, engaging, and easy to interpret. Export options will also be provided, allowing reports to be downloaded in PDF format for academic evaluation or self-reflection.

The implementation strategy will follow an iterative development cycle, beginning with prototyping and limited pilot testing. Feedback from these stages will be used to refine the CNN models, NLP modules, and user interface. Larger-scale deployment will follow, with integration into university training environments and communication skills programs. By adopting this phased and feedback-driven approach, the solution will be continuously improved to meet the needs of its users.

Overall, the software solution approach ensures that the Emotionally-Aware Presentation Analyzer is not only a technically advanced tool but also one that addresses the cultural and educational requirements of Sri Lankan undergraduates. Through data-driven design, deep learning techniques, and localized adaptation, the system will provide a meaningful and scalable solution for developing authentic and effective presentation skills.



•	Real-Time Feedback System:
The Analyzer will incorporate a real-time feedback mechanism that provides immediate visual cues to presenters during their delivery. This system will notify users if their face is not fully visible within the camera frame, if lighting conditions hinder recognition, or if their emotional expressions appear inconsistent with the expected tone of the presentation. By receiving these instant alerts, students can adjust their posture, visibility, or delivery style while still presenting, ensuring stronger audience engagement and improved overall effectiveness.

•	Model Evaluation:
To ensure reliability and accuracy, the performance of the trained convolutional neural network models will be systematically evaluated. Metrics such as precision, recall, F1 score, and overall accuracy will be used to measure the system’s ability to correctly classify emotions across different conditions. The evaluation process will also test the robustness of the model with locally adapted datasets to minimize cultural bias in recognizing Sri Lankan students’ facial expressions. By validating the model against both controlled datasets and real-world presentation scenarios, the system ensures dependable performance that supports authentic feedback and long-term learning objectives.

•	User Interface and Experience:
The user interface will be designed to provide an engaging and intuitive experience for undergraduates using the Analyzer. The dashboard will present real-time alerts, visibility checks, and emotional alignment indicators in a clear and non-intrusive manner. At the conclusion of each presentation, students will be able to access detailed reports that visualize their emotional timeline, visibility percentages, and congruence scores through charts and heatmaps. The design will prioritize clarity, accessibility, and ease of interpretation, ensuring that users can readily understand their strengths and areas for improvement. By integrating interactive elements with modern UI principles, the interface will encourage active engagement and reflection, helping students enhance both their emotional awareness and presentation delivery skills.


